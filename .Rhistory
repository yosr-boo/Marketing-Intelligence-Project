scan <- ds$scanner(batch_size = 1e5)
scan <- data_3$scanner(batch_size = 1e5)
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(data.table)
library(tidyr)
library(duckdb)
library(DBI)
library(ggplot2)
library(lubridate)
setwd("C:/Users/Yoser/Masters/Ca'Foscari/2 anno/Marketing Analytics/Parquet files")
data <- read_parquet("final_data3.parquet")
prepare_data <- function(df) {
df %>%
mutate(
# Extract hour, day of week, month
pickup_hour = hour(tpep_pickup_datetime),
pickup_dow  = wday(tpep_pickup_datetime, week_start=1, label=TRUE, abbr=FALSE), # Monday=1, Sunday=7
pickup_month = month(tpep_pickup_datetime, label=TRUE, abbr=FALSE),
# Convert location IDs, rate code, payment type to factors
pickup_location = as.factor(PULocationID),
dropoff_location = as.factor(DOLocationID),
ratecodeid = as.factor(RatecodeID),
payment_type = as.factor(payment_type)
)
}
# 1. Install and Load Necessary Packages -------------------------------------
install.packages("glmnet")  # If not installed
library(glmnet)
# 3. Prepare Data ----------------------------------------------------------
df_prepared <- prepare_data(data)  # Your preprocessing function
# Convert Factor Variables to Numeric (Needed for glmnet's model.matrix)
df_prepared <- as.data.table(df_prepared)
df_prepared[, pickup_location := as.numeric(as.factor(pickup_location))]
df_prepared[, dropoff_location := as.numeric(as.factor(dropoff_location))]
df_prepared[, ratecodeid := as.numeric(as.factor(ratecodeid))]
df_prepared[, payment_type := as.numeric(as.factor(payment_type))]
df_prepared[, payment_type := as.numeric(as.factor(payment_type))]
# 4. Train-test split ----------------------------------------------------
set.seed(123)
train_indices <- sample(nrow(df_prepared), size = 0.7 * nrow(df_prepared))
train_data <- df_prepared[train_indices, ]
test_data  <- df_prepared[-train_indices, ]
# 5. Define Model Formula ------------------------------------------------
formula <- operational_margin ~ pickup_hour + pickup_dow + pickup_month +
pickup_location + dropoff_location +
ratecodeid + payment_type
# 6. Process Training Data in Chunks to Build Design Matrix -----------------
chunk_size <- 1e5
x_train <- NULL
y_train <- NULL
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Create design matrix for the current chunk; exclude the intercept column if desired
x_chunk <- model.matrix(formula, data = chunk)[, -1]
y_chunk <- chunk$operational_margin
# Combine chunks
if (is.null(x_train)) {
x_train <- x_chunk
y_train <- y_chunk
} else {
x_train <- rbind(x_train, x_chunk)
y_train <- c(y_train, y_chunk)
}
}
# 1. Install and Load Necessary Packages -------------------------------------
install.packages("sgd")         # For stochastic gradient descent
library(sgd)
# Process training data in chunks
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
# Extract the current chunk and convert to a data.frame
chunk <- as.data.frame(train_data[i:min(i + chunk_size - 1, nrow(train_data)), ])
# For the first chunk, use default initialization; for later chunks, use the previous coefficients
if (is.null(model_coeff)) {
sgd_model <- sgd(formula, data = chunk, model = "lm")
} else {
sgd_model <- sgd(formula, data = chunk, model = "lm",
sgd.control = list(initial = model_coeff))
}
# Save the updated coefficients for the next chunk
model_coeff <- coef(sgd_model)
}
model_coeff <- NULL  # To hold coefficients from previous chunks
sgd_model <- NULL    # To store the current sgd model object
# Process training data in chunks
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
# Extract the current chunk and convert to a data.frame
chunk <- as.data.frame(train_data[i:min(i + chunk_size - 1, nrow(train_data)), ])
# For the first chunk, use default initialization; for later chunks, use the previous coefficients
if (is.null(model_coeff)) {
sgd_model <- sgd(formula, data = chunk, model = "lm")
} else {
sgd_model <- sgd(formula, data = chunk, model = "lm",
sgd.control = list(initial = model_coeff))
}
# Save the updated coefficients for the next chunk
model_coeff <- coef(sgd_model)
}
# Process training data in chunks
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
# Extract the current chunk and convert to a data.frame
chunk <- as.data.frame(train_data[i:min(i + chunk_size - 1, nrow(train_data)), ])
# For the first chunk, use default initialization; for later chunks, use the previous coefficients
if (is.null(model_coeff)) {
sgd_model <- sgd(formula, data = chunk, model = "lm")
} else {
sgd_model <- sgd(formula, data = chunk, model = "lm",
sgd.control = list(start = model_coeff))
}
# Save the updated coefficients for the next chunk
model_coeff <- coef(sgd_model)
}
# 2. Check Memory Usage Before Running Model -------------------------------
gc()  # Run garbage collection
# 6. Fit Model Using SGD in Chunks ----------------------------------------
chunk_size <- 5e4
# Process training data in chunks
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
# Extract the current chunk and convert to a data.frame
chunk <- as.data.frame(train_data[i:min(i + chunk_size - 1, nrow(train_data)), ])
# For the first chunk, use default initialization; for later chunks, use the previous coefficients
if (is.null(model_coeff)) {
sgd_model <- sgd(formula, data = chunk, model = "lm")
} else {
sgd_model <- sgd(formula, data = chunk, model = "lm",
sgd.control = list(start = model_coeff))
}
# Save the updated coefficients for the next chunk
model_coeff <- coef(sgd_model)
}
# 2. Check Memory Usage Before Running Model -------------------------------
gc()  # Run garbage collection
# Example skeleton
for (i in seq(1, nrow(train_data), by = chunk_size)) {
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Construct minimal matrix
x_chunk <- cbind(
1,  # intercept
chunk$pickup_hour,
chunk$pickup_dow,
chunk$pickup_month,
chunk$pickup_location,
chunk$dropoff_location,
chunk$ratecodeid,
chunk$payment_type
)
y_chunk <- chunk$operational_margin
# Update the model with sgd
if (is.null(model_coeff)) {
sgd_model <- sgd(x = x_chunk, y = y_chunk, model = "lm")
} else {
sgd_model <- sgd(
x = x_chunk,
y = y_chunk,
model = "lm",
sgd.control = list(start = model_coeff)
)
}
model_coeff <- coef(sgd_model)
# Clean up
rm(chunk, x_chunk, y_chunk)
gc()
}
# Create a template to capture all factor levels
template <- model.matrix(formula, data = train_data[1:min(100, nrow(train_data)), ])
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Use the xlev attribute from the template to ensure consistent columns
x_chunk <- model.matrix(formula, data = chunk, xlev = attr(template, "xlevels"))
y_chunk <- chunk$operational_margin
# (Update your sgd call as needed, using the consistent design matrix)
}
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Manually create the design matrix with a constant intercept column
x_chunk <- cbind(
intercept      = 1,
pickup_hour    = chunk$pickup_hour,
pickup_dow     = chunk$pickup_dow,
pickup_month   = chunk$pickup_month,
pickup_location= chunk$pickup_location,
dropoff_location = chunk$dropoff_location,
ratecodeid     = chunk$ratecodeid,
payment_type   = chunk$payment_type
)
y_chunk <- chunk$operational_margin
# Now x_chunk will always have 8 columns.
# Use x_chunk and y_chunk in your sgd call, and update your model.
if (is.null(model_coeff)) {
sgd_model <- sgd(x = x_chunk, y = y_chunk, model = "lm")
} else {
sgd_model <- sgd(
x = x_chunk,
y = y_chunk,
model = "lm",
sgd.control = list(start = model_coeff)
)
}
model_coeff <- coef(sgd_model)
# Clean up
rm(chunk, x_chunk, y_chunk)
gc()
}
# 6. Fit Model Using SGD in Chunks ----------------------------------------
chunk_size <- 5e4
model_coeff <- NULL  # To hold coefficients from previous chunks
sgd_model <- NULL    # To store the current sgd model object
# 2. Check Memory Usage Before Running Model -------------------------------
gc()  # Run garbage collection
# 6. Fit Model Using SGD in Chunks ----------------------------------------
chunk_size <- 5e4
model_coeff <- NULL  # To hold coefficients from previous chunks
sgd_model <- NULL    # To store the current sgd model object
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Manually create the design matrix with a constant intercept column
x_chunk <- cbind(
intercept      = 1,
pickup_hour    = chunk$pickup_hour,
pickup_dow     = chunk$pickup_dow,
pickup_month   = chunk$pickup_month,
pickup_location= chunk$pickup_location,
dropoff_location = chunk$dropoff_location,
ratecodeid     = chunk$ratecodeid,
payment_type   = chunk$payment_type
)
y_chunk <- chunk$operational_margin
# Now x_chunk will always have 8 columns.
# Use x_chunk and y_chunk in your sgd call, and update your model.
if (is.null(model_coeff)) {
sgd_model <- sgd(x = x_chunk, y = y_chunk, model = "lm")
} else {
sgd_model <- sgd(
x = x_chunk,
y = y_chunk,
model = "lm",
sgd.control = list(start = model_coeff)
)
}
model_coeff <- coef(sgd_model)
# Clean up
rm(chunk, x_chunk, y_chunk)
gc()
}
x_test <- as.matrix(test_data[, features, with = FALSE])
# Print the column names in test_data
print(colnames(test_data))
# Print the features vector
print(features)
features <- c(
"pickup_hour",
"pickup_dow",
"pickup_month",
"pickup_location",
"dropoff_location",
"ratecodeid",
"payment_type"
)
x_test <- as.matrix(test_data[, features, with = FALSE])
gc()
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(data.table)
library(tidyr)
library(duckdb)
library(DBI)
library(ggplot2)
library(lubridate)
setwd("C:/Users/Yoser/Masters/Ca'Foscari/2 anno/Marketing Analytics/Parquet files")
data <- read_parquet("final_data3.parquet")
prepare_data <- function(df) {
df %>%
mutate(
# Extract hour, day of week, month
pickup_hour = hour(tpep_pickup_datetime),
pickup_dow  = wday(tpep_pickup_datetime, week_start=1, label=TRUE, abbr=FALSE), # Monday=1, Sunday=7
pickup_month = month(tpep_pickup_datetime, label=TRUE, abbr=FALSE),
# Convert location IDs, rate code, payment type to factors
pickup_location = as.factor(PULocationID),
dropoff_location = as.factor(DOLocationID),
ratecodeid = as.factor(RatecodeID),
payment_type = as.factor(payment_type)
)
}
library(sgd)
library(data.table)  # Efficient data processing
# 2. Check Memory Usage Before Running Model -------------------------------
gc()  # Run garbage collection
# 3. Prepare Data ----------------------------------------------------------
df_prepared <- prepare_data(data)  # Your preprocessing function
# Convert Factor Variables to Numeric (Needed for our model matrix)
df_prepared <- as.data.table(df_prepared)
df_prepared[, pickup_location := as.numeric(as.factor(pickup_location))]
df_prepared[, dropoff_location := as.numeric(as.factor(dropoff_location))]
df_prepared[, ratecodeid := as.numeric(as.factor(ratecodeid))]
df_prepared[, payment_type := as.numeric(as.factor(payment_type))]
# 4. Train-test split ----------------------------------------------------
set.seed(123)
train_indices <- sample(nrow(df_prepared), size = 0.7 * nrow(df_prepared))
train_data <- df_prepared[train_indices, ]
test_data  <- df_prepared[-train_indices, ]
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Manually create the design matrix with a constant intercept column
x_chunk <- cbind(
intercept      = 1,
pickup_hour    = chunk$pickup_hour,
pickup_dow     = chunk$pickup_dow,
pickup_month   = chunk$pickup_month,
pickup_location= chunk$pickup_location,
dropoff_location = chunk$dropoff_location,
ratecodeid     = chunk$ratecodeid,
payment_type   = chunk$payment_type
)
y_chunk <- chunk$operational_margin
# Now x_chunk will always have 8 columns.
# Use x_chunk and y_chunk in your sgd call, and update your model.
if (is.null(model_coeff)) {
sgd_model <- sgd(x = x_chunk, y = y_chunk, model = "lm")
} else {
sgd_model <- sgd(
x = x_chunk,
y = y_chunk,
model = "lm",
sgd.control = list(start = model_coeff)
)
}
model_coeff <- coef(sgd_model)
# Clean up
rm(chunk, x_chunk, y_chunk)
gc()
}
# 6. Fit Model Using SGD in Chunks ----------------------------------------
chunk_size <- 5e4
model_coeff <- NULL  # To hold coefficients from previous chunks
sgd_model <- NULL    # To store the current sgd model object
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Manually create the design matrix with a constant intercept column
x_chunk <- cbind(
intercept      = 1,
pickup_hour    = chunk$pickup_hour,
pickup_dow     = chunk$pickup_dow,
pickup_month   = chunk$pickup_month,
pickup_location= chunk$pickup_location,
dropoff_location = chunk$dropoff_location,
ratecodeid     = chunk$ratecodeid,
payment_type   = chunk$payment_type
)
y_chunk <- chunk$operational_margin
# Now x_chunk will always have 8 columns.
# Use x_chunk and y_chunk in your sgd call, and update your model.
if (is.null(model_coeff)) {
sgd_model <- sgd(x = x_chunk, y = y_chunk, model = "lm")
} else {
sgd_model <- sgd(
x = x_chunk,
y = y_chunk,
model = "lm",
sgd.control = list(start = model_coeff)
)
}
model_coeff <- coef(sgd_model)
# Clean up
rm(chunk, x_chunk, y_chunk)
gc()
}
features <- c(
"pickup_hour",
"pickup_dow",
"pickup_month",
"pickup_location",
"dropoff_location",
"ratecodeid",
"payment_type"
)
x_test <- as.matrix(test_data[, features, with = FALSE])
dtest <- xgb.DMatrix(data = x_test)
install.packages("xgboost")
library(xgboost)
dtest <- xgb.DMatrix(data = x_test)
# 1. Build the Design Matrix for the Test Set
# (Make sure the test set has the same columns and preprocessing as the training set)
x_test <- cbind(
intercept        = 1,
pickup_hour      = test_data$pickup_hour,
pickup_dow       = test_data$pickup_dow,
pickup_month     = test_data$pickup_month,
pickup_location  = test_data$pickup_location,
dropoff_location = test_data$dropoff_location,
ratecodeid       = test_data$ratecodeid,
payment_type     = test_data$payment_type
)
x_test <- as.matrix(x_test)
# 2. Generate Predictions Using the Final sgd Model
preds <- predict(sgd_model, newdata = x_test)
# 3. Calculate the RMSE to Assess Performance
actuals <- test_data$operational_margin
rmse <- sqrt(mean((preds - actuals)^2))
cat("RMSE on test set:", rmse, "\n")
coef(sgd_model)
summary(sgd_model)
print(coef(sgd_model))
summary(sgd_model)
# 4. Define Features -------------------------------------------------------
features <- c("pickup_hour", "pickup_dow", "pickup_month",
"pickup_location", "dropoff_location",
"ratecodeid", "payment_type")
# 5. Incremental Training with xgboost ------------------------------------
chunk_size <- 1e5
xgb_model <- NULL  # Will hold the continuously updated model
# Set basic xgboost parameters
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
# Process the training data in chunks
for (i in seq(1, nrow(train_data), by = chunk_size)) {
cat("Processing rows:", i, "to", min(i + chunk_size - 1, nrow(train_data)), "\n")
# Subset the current chunk
chunk <- train_data[i:min(i + chunk_size - 1, nrow(train_data)), ]
# Build the design matrix (predictors) and target vector
x_chunk <- as.matrix(chunk[, ..features])
y_chunk <- chunk$operational_margin
# Create a DMatrix for this chunk
dtrain <- xgb.DMatrix(data = x_chunk, label = y_chunk)
# Train on the current chunk, warm starting from previous rounds if available.
# You can adjust nrounds per chunk as needed.
nrounds <- 10
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
xgb_model = xgb_model  # This continues training from the previous model
)
# Clean up to free memory
rm(chunk, x_chunk, y_chunk, dtrain)
gc()
}
# Convert all feature columns to numeric in the entire dataset
df_prepared[, (features) := lapply(.SD, function(col) as.numeric(as.factor(col))), .SDcols = features]
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
library(dplyr)
library(data.table)
library(tidyr)
library(duckdb)
library(DBI)
library(ggplot2)
library(lubridate)
#install.packages("biglm")
install.packages("sgd")         # For stochastic gradient descent
library(sgd)
library(biglm)
library(sgd)
library(xgboost)
setwd("C:/Users/Yoser/Masters/Ca'Foscari/2 anno/Marketing Analytics/Parquet files")
data_2024 <- read_parquet("final_clean24")
data_2024 <- read_parquet("final_clean24")
setwd("C:/Users/Yoser/Masters/Ca'Foscari/2 anno/Marketing Analytics/Parquet files")
setwd("C:/Users/Yoser/Masters/Ca'Foscari/2 anno/Marketing Analytics/Parquet files")
data_2024 <- read_parquet("final_clean24")
data_2024 <- read_parquet("final_clean24.parquet")
data <- read_parquet("final_data3.parquet")
data_subset <- data %>% select(tpep_pickup_datetime, PULocationID, DOLocationID, passenger_count,
RatecodeID, payment_type, operational_margin)
prepare_data <- function(df) {
df %>%
mutate(
# Extract hour, day of week, month
pickup_hour = hour(tpep_pickup_datetime),
pickup_dow  = wday(tpep_pickup_datetime, week_start=1, label=TRUE, abbr=FALSE), # Monday=1, Sunday=7
pickup_month = month(tpep_pickup_datetime, label=TRUE, abbr=FALSE),
# Convert location IDs, rate code, payment type to factors
pickup_location = as.factor(PULocationID),
dropoff_location = as.factor(DOLocationID),
ratecodeid = as.factor(RatecodeID),
payment_type = as.factor(payment_type)
)
}
# Check Memory Usage Before Running Model -------------------------------
gc()  # Run garbage collection
cat("Available RAM:", memory.size(), "MB\n")
cat("Max RAM Limit:", memory.limit(), "MB\n")
# Prepare Data ----------------------------------------------------------
df_prepared <- prepare_data(data_subset)  # Your preprocessing function
# Convert Factor Variables to Numeric (Needed for biglm)
df_prepared <- as.data.table(df_prepared)
df_prepared[, pickup_location := as.numeric(as.factor(pickup_location))]
df_prepared[, dropoff_location := as.numeric(as.factor(dropoff_location))]
df_prepared[, ratecodeid := as.numeric(as.factor(ratecodeid))]
df_prepared[, payment_type := as.numeric(as.factor(payment_type))]
# Convert all feature columns to numeric in the entire dataset
df_prepared[, (features) := lapply(.SD, function(col) as.numeric(as.factor(col))), .SDcols = features]
data_subset_2024 <- data_2024 %>% select(tpep_pickup_datetime, PULocationID, DOLocationID, passenger_count,
RatecodeID, payment_type, operational_margin)
data_subset_2024 <- data_2024 %>% select(tpep_pickup_datetime, pulocationid, dolocationid, passenger_count,
ratecodeid, payment_type)
# Prepare Data ----------------------------------------------------------
df_prepared_2024 <- prepare_data(data_subset_2024)  # Your preprocessing function
